I did this project by myself: Oren Sultan.
I used python version 3.5
Questions:
1. 

What I did in order to answer this question, I changed the data set of breast_cancer in different ways:
1. source data 'no-recurrence-events' is 70% of the classes and 'recurrence-events' 30%
2. extreme dominant class for example all of instances are 'no-recurrence-events'
3. normal distribution 50% of instances 'no-recurrence-events' and 50% 'recurrence-events'

-We expect the prior probabilities to be close to 50% for each class (because each class is a sum of probabilities that chose randomely).
Indeed I confirmed it in we most of the time will get priors in range 45%-55% for each class.

now,
- if data in normal distribution (actual 50%-50%) we will always get accuracy in the initial phase of close to 50% and after iterations also close to 50%(because the priors and the actual are really very close)

- if it's exterme dominant class for example all of classes 'no-recurrence-events'.
as the prior of 'no-recurrence-events' is higher than 50%, the accuracy of initial phase will be better. e.g
true negative, false positive: [204, 82]
 false negative, true positive: [0, 0]
accuracy in percents % : 71

What I noticed is that when you have exterme dominant class you are more sensible to priors which are far from the truth. for example 'recurrence-events' will get prior 0.9 and the 
'no-recurrence-events' will get 0.1.
true negative, false positive: [0, 286]
 false negative, true positive: [0, 0]
-> 0/286 = 0% (of course with swapping we will get 100% accuracy)

On the other hand if your data is distributed 50%-50% between classes so if the priors you get are very far from 50% . e.g 0.95 and 0.05 (doesnt matter which class get which pr) it will be close to 50% accuracy. 

I have no math proofs for all of it by I thought about it alone and confirmed it.

Why does it work?
First we have the iterations of update of probabilities which is implemented and thanks to it we can get better probabilities we are more accurate than random, and converge to a better accuracy in the end.
In addition I implemented the swapping (just take the higest sum of diagonal in my 2*2 confusion matrix, and then we stick to the class with higher probability after iteration 0 after swapping if needed)

In addition, we have dominant class which is 'no-recurrence-events' (it's not so dominant but still 70% is relatively higher than normal of 50% dist) that's what give us the oppertunity to get accuracy which is much higher than 50% in the end, in contrast to distribution of 50%-50% that as I demonstrated will give us very close to 50% accuracy.


When it will fail?
We choose the initial prior randomely, if prior probabilities that will be choose are very far from the actual, for example we have class A and class B and the actual is P(A) = 0.99 and P(B) = 0.01 if we will change it to be P(A) = 0.01 and P(B) = 0.99 (we took the complement prob.)
so without swapping the initial phase will give for first iteration very bad.
for example:
for 
priors_pr[first_class_pr] = 0.99
priors_pr[second_class_pr] = 0.01
true negative, false positive: [0, 201]
false negative, true positive: [0, 85]
so we have (0+85)/286 = 85/286 = 30%

and as we have more dominant class it will be more sensible - more severe.

so for dataset with low number of instances (we have only 286) it will be hard to converge for better accuracy also if we have iterations.




4. I implemented add-k smoothing, the method in the code is called "calc_add_k_smoothing"
and I run it on supervised model on breast-cancer and I notice differences such as
K is more larger it may damage the accuracy and make it lower accuracy.
for example:
k = 1 , (prediction on all instances, one evaluation) gives accuracy = 75%
and as k is higher, the accuracy is lower.
( k = 10, accuracy = 74%
  k = 100, accuracy = 72%
  ...
)


I also noticed that there is some limit how it can affect the accuracy to be good or bad.
for example no matter what is k, I can't get better accuracy than 75%.
So yes I see a change from the original implementation which is without smoothing.
the change is depends on k, if it's a very large k I will see much more dramatic difference.

Explaination:
when K is large it assigns too much probability to unseen events, that's why the result we get may be a distortion of the reality. as k is larger the calculation will be less accuracte.
On the other hand if k is small enough (like epsilon) the effect will be neglegible, so it will give 0.00001 probability for a 0 probability for example, that's why the result will be very close
to reality.
In addition, in this data set zero-probability is rare, there is no much different between add-k with small k (1 for example) to implementation without smoothing.
also the accuracy without smoothing is 75% as I got with add-1 smoothing.

To conclude:
if in your data set you may get a lot of zero - probabilities, use smoothing with low K it will improve your accuracy, if no zero-probabilities avoid use smoothing because you will not get from his advantage and you are exposed to his disadvantages.

5. 

I implemented a method which is called deterministically_init() to check it
you should assign det_init = True.

confirmation:

As we can see the accuracy close to 50% in each run (in range 45%-55% most of the time)
And we have seen that the unsupervised NB will give us 65-70% most of the time (which is at most 
10% less than the supervised). So, we see that deterministically_init is most of the time
worse than the guessing method of probability of the instances which we implemented.
So confirmation OK.

demonstration:

Since we choose randomely the class for each instance we expect to get approx 50% instances with
class 'recurrence-events'and 50% with class 'no-recurrence-events'.
But in reality our data is not distribute to classes 50%, 50%.
in breast-cancer for example (201 of 286 instances belongs to the class 'no-recurrence-events'
and 85 to 'recurrence-events'), in addition also if we are correct about distribution of 
instances to classes, let's assume reality is also close to 50%, 50% so we have expectation
to be right in our guess on half of the instances (accuracy close to 50%)

if for example for class 'no-recurrence-events' we got more than 50% so the accuracy suppose to be higher because most of the instances actual class is 'no-recurrence-events' (201 in 286)
and if less than 50% so the accuracy suppose to be worse.
- I say suppose to be because, the number of 'no-recurrence-events' is increased, but maybe we have bad luck and we gave to more wrong instances the class 'no-recurrence-events' and they 
actually 'recurrence-events'.

now, probability of instance class to be like actual_class = 
the probability the actual and the instance class we choose randomly are 'no-recurrence-events'
+
the probability the actual and the instance class we choose randomly are 'recurrence-events'

(it's actually union between groups but because of disjoint set : P(A union B) = P(A)+P(B))

P(actual_class = 'no-recurrence-events', instance_class = 'no-recurrence-events') =

201/286 * 1/2 = 0.35

P(actual_class = 'recurrence-events', instance_class = 'recurrence-events') =
100/286 * 1/2 = 0.17
- I can multiply the probabilities because of independence, I choose instance class randomly

P(actual_class = 'no-recurrence-events', instance_class = 'no-recurrence-events')
+ P(actual_class = 'recurrence-events', instance_class = 'recurrence-events') = 0.35 + 0.17 = 0.52 = probability of instance class to be like actual_class

so, the distribution of classes is close to 50% instances class 'no-recurrence-events' and
50% 'recurrence-events' we will be close to 52% accuracy according to this data set.

So what I demonstrated is that we will get accuracy which is close to 50% (most of the time in range 45%-55%) and that's why we will prefer not to choose it deterministically.


